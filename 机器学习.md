#  机器学习

机器学习指**利用算法识别一组数据中的模式并做出预测**



机器学习的主要挑战是：

- 缺乏数据
- 数据质量不佳
- 数据代表性不强，信息价值小
- 简单的模型欠拟合，复杂的模型过拟合

# 机器学习过程

首先进行**EDA探索性数据分析**，即绘图分析自变量对因变量的影响，比如找出年龄在20-40之间的人存活率更高，那么在特征工程中将连续型的年龄变量划分为多段，其中20-40为一段

随后进行特征工程，填补缺失值和修正异常值，特征选择，特征转换(one hot编码)，特征构造

模型选择，建立多个模型，搜索超参数选择最好的

预测结果

# 代码注意事项

从n,m维的矩阵中取出的X[:,idx]或者X[idx,:]**默认是（n,）**

遍历（n,1）维的数组时，需要用**ravel**展开

注意pandas读取数据的类型，如果是uint8类型计算时会导致**溢出**，需要进行转换**X = X.astype(np.float32)**



# 常用库函数

## 优化函数

```python
from scipy.optimize import minimize
'''
fun 目标函数
x0 初始猜测值，也是fun的第一个参数
args 传递给fun的额外参数
jac fun是否会返回梯度，True表示会返回，且fun要返回(f, grad)形式，如果不返回则自行提供一个梯度函数
options 额外配置比如如最大迭代次数

'''
fmin=opt.minimize(fun=back_propagate,x0=params,args=(input_size,hidden_size,num_label,y_hot,learning_rate),
                  method='TNC',jac=True,options={'maxiter':250})
```

# 数据可视化

除了基本的pd.info() pd.describe()之类的，可以通过ProfileReport更快的展示数据

```python
from ydata_profiling import ProfileReport
profile = ProfileReport(train, title="Train Data Profiling Report", explorative=True)

# 在Notebook中展示报告
profile.to_notebook_iframe()
```





# 特征工程

## 数据预处理

### 缺失值

对于缺失值：

+ 删除缺失值
  + 删除**实例**
  + 删除**特征**
+ 填充缺失值
  + **固定值，均值，中位数，众数**
  + 作为**None**，比如泳池质量的确实代表不存在泳池
  + **Random Forest**填充
  + **KNN** 填充

### 异常值



对于异常值：

+ 删除异常值
  + **绘图**，找出异常值的条件删除



### 归一化

**归一化可以提高模型收敛速度，提高模型精度，防止模型训练过程中发生数值计算错误，未经归一化处理的数据在搜索最优解的过程中可能会陷入缓慢前进或者模型震荡的状态，因为每个特征对模型的影响力不同，导致优化路径十分曲折。**





对于依赖距离内积，梯度的模型，需要标准化

| 模型类别 | 代表模型                                   | 是否需要标准化 |
| -------- | ------------------------------------------ | -------------- |
| 线性模型 | LinearRegression、Ridge、Lasso、ElasticNet | ✅ 需要         |
| SVM      | SVC, SVR                                   | ✅ 需要         |
| KNN      | KNeighborsClassifier, KNeighborsRegressor  | ✅ 需要         |
| PCA/降维 | PCA, LDA                                   | ✅ 需要         |
| 神经网络 | MLPRegressor, MLPClassifier                | ✅ 需要         |
| 聚类     | KMeans                                     | ✅ 需要         |



模型内部有排序分裂等机制，对数值大小不敏感

| 模型类别 | 代表模型                    | 是否需要标准化                   |
| -------- | --------------------------- | -------------------------------- |
| 树模型   | DecisionTree, RandomForest  | ❌ 不需要                         |
| 提升模型 | XGBoost, LightGBM, CatBoost | ❌ 通常不需要（但有时仍建议试试） |



此外ElasticNet和Lasso对**异常值**敏感，需要RobustScaler处理



**StandardScaler**(Z-score标准化)，将数据转换为**均值为0**，**方差为1**

<img src="./assets/image-20250408104412478.png" alt="image-20250408104412478" style="zoom:50%;" />

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```



RobustScaler使用**中位数**和**四分位间距**

<img src="./assets/image-20250408104559279.png" alt="image-20250408104559279" style="zoom:50%;" />

```python
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
```



可以借助pipeline流程化需要预处理的模型

```python
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import Lasso

model = make_pipeline(RobustScaler(), Lasso(alpha=0.001))
```



### 独热编码

在分类，回归，聚类等学习算法中，特征之间**距离**的计算或者**相似度**的计算是十分重要的，独热编码将离散特征的取值扩展到了**欧式空间**，可以让特征之间的计算更加合理，注意要求特征不属于**有序变量**，如：`教育程度`（小学 < 初中 < 高中 < 大学）、`质量等级`（差、中、好）

比如有一个离散型特征表示工作类型 $$x_1=1,x_2=2,x_3=3$$，这些信息之间不存在**顺序**

在计算两个工作之间的距离时$$(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2$$，这样计算出的特征是不合理的，转换为**独热编码**后求解出的两个工作之间的距离是一样的

 



将分类变量转换为**二进制**表示

**<img src="./assets/image-20250408105941801.png" alt="image-20250408105941801" style="zoom:67%;" />**



pd.get_dummies()用独热编码代替**object** 或 **category** 类型，并**删除**原来的列，对于**int类型**的离散型数据可以先**手动**转换为string类型

```python
import pandas as pd

df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue']})
pd.get_dummies(df)

   Color_Blue  Color_Green  Color_Red
0           0            0          1
1           0            1          0
2           1            0          0
```



### 偏态数据

对于假设数据满足正态分布的模型——回归模型，线性模型，当数据严重偏态时需要进行转换



首先绘制图像判断是否偏态(skewness)

```python
sns.histplot(train['SalePrice'],kde=True)
```

<img src="./assets/image-20250408111902742.png" alt="image-20250408111902742" style="zoom:80%;" />

该图出现的情况是**右偏**(正偏）**均值 > 中位数 > 众数**，房价，收入等，大部分数据集中在**左侧**，而较少的观测值**远离中心**，使得右尾部比左尾部**更长**



绘制Q-Q图，比较两个分布的分位数来判断是否相似，横轴是理论分布(正态分布)，纵轴是样本数据的分位数。

```python
from scipy.stats import norm,skew
stats.probplot(train['SalePrice'],plot=ax[1])
```

<img src="./assets/image-20250408112348535.png" alt="image-20250408112348535" style="zoom:67%;" />

排列接近直线——正态分布，尾部上扬——数据右偏 ，尾部下垂——数据左偏 



对于右偏数据一般进行对数处理(非负数)、

或者**boxcox**转换

<img src="./assets/image-20250408121805981.png" alt="image-20250408121805981" style="zoom:50%;" />

```python
train['SalePrice']=np.log1p(train['SalePrice']) # log(1+x)

from scipy.special import boxcox1p
boxcox1p(x, λ)
```



处理偏态数据

```python
skewness=skewness[abs(skewness)>0.75]
skewed_features=skewness.index
lam=0.15
for feat in skewed_features:
    full[feat]=boxcox1p(full[feat],lam)
```





# 基础概念



## 超参数

超参数是在**训练机器学习模型之前**手动设置的参数，它们**不会在训练中自动更新**，而是由人工或自动优化方法得到的

比如学习率，正则化参数，神经元个数

## 监督学习

监督学习是指**给算法一个数据集，并且给定正确的答案，算法的目标是给出更多的正确答案**。

监督学习有两类问题：其中一个是**回归问题**，**预测连续的输出值**；另一个是**分类问题**，**推测出离散的输出值**。



## 无监督学习

无监督学习就是给定一个无标签的数据集，从中找出某种数据结构。比如从音频中分离出人声和背景声。



## 代价函数

拟合的目标是选择合适的参数使得代价平方误差函数最小。

对于回归问题，最基本的代价函数是**均方误差损失函数**

![image-20250317090312274](./assets/image-20250317090312274.png)



一阶拟合函数的代价函数变化如下

<img src="./assets/image-20250317091709849.png" alt="image-20250317091709849" style="zoom:67%;" />

<img src="./assets/image-20250317092837613.png" alt="image-20250317092837613" style="zoom:67%;" />



线性回归的代价函数是**convex function**



<img src="./assets/764050-20180610160817960-1620976826.png" alt="img" style="zoom:67%;" />



<img src="./assets/764050-20180610161045364-1903751756.png" alt="img" style="zoom:67%;" />

因此梯度下降一定可以得到**全局最优解**





## 交叉验证

交叉验证是一种评估机器学习模型泛化能力的方法，将数据集划分为多个子集，多次训练和测试模型



最常见的是**K折交叉验证（K-Fold Cross-Validation）**

1. 将原始数据集划分为$k$个大小相近的子集（folds）；
2. 每次从$k$个子集中选一个作为**验证集**，其余$k-1$个作为**训练集**；
3. 训练并评估模型，共进行$k$次；
4. 将$k$次的验证结果（如准确率、MSE）**取平均**，作为模型的性能指标。





## 偏差和方差

当运行学习算法时如果结果不理想，多半是出现了**高偏差(bias)或者是高方差(Variance)**，也就说欠拟合和过拟合。

偏差：**模型预测的期望值与真实值的差距**，方差：**不同样本下模型预测结果的波动程度**。注意数学上方差的计算公式是

<img src="./assets/image-20250401123608401.png" alt="image-20250401123608401" style="zoom:67%;" />





**当验证集误差接近训练集误差时——偏差/欠拟合**

**当验证集误差远远大于训练集误差时——方差/过拟合**



1. **获得更多的训练实例——解决高方差** 
2. **尝试减少特征的数量——解决高方差** 
3. **尝试获得更多的特征——解决高偏差** 
4. **尝试增加多项式特征——解决高偏差** 
5. **尝试减少正则化程度 λ——解决高偏差** 
6. **尝试增加正则化程度 λ——解决高方差**



代码见[ex5](./个人代码实现/ex5.html)

## 过拟合问题

通过引入**高阶项以及其他许多的特征**可以很好的拟合训练数据，但是训练出的模型**推广**到新的数据的效果并不好。



![image-20250320130300601](./assets/image-20250320130300601.png)

![image-20250320130305939](./assets/image-20250320130305939.png)



当出现了过拟合问题时，可以通过以下方法解决

+ 获取更多的数据
+ 简化模型：减少模型的参数，比如说神经网络中每层的节点数以及层数
+ 降维
+ 正则化

# 梯度下降

## 批量梯度下降(BGD)

梯度下降是一种用于求函数最小值的算法，其背后的思想是：**初始时选择一个任意的参数组合计算代价函数，然后寻找下一个能让代价函数下降最多的参数组合，一直重复操作直到到达一个局部最小值，初始时选择不同的参数可能会得到不同的全局最小值。**

![image-20250317094537136](./assets/image-20250317094537136.png)

批量梯度下降（batch gradient descent）算法的公式为：

![](./assets/image-20250317094734225.png)

展开有

![image-20250317103223998](./assets/image-20250317103223998.png)

<img src="./assets/image-20250317103236273.png" alt="image-20250317103236273" style="zoom: 80%;" />



注意要同时更新两个参数值，这里的a是**学习率**，代表每一次下降的大小。根据公式可以看出每一次更新参数值都会遍历一遍所有的数据，所以速度会很慢。

如果目标函数为**凸函数**，一定可以收敛到**全局最优**。

<img src="./assets/image-20250317100242305.png" alt="image-20250317100242305" style="zoom:67%;" />

如果学习率太**小**会需要**很多步来逼近最小值**，如果学习率太**大**，可能会不**断越过最小点导致无法收敛**，经过证明的是，只要学习率足够小，代价会在每一轮迭代中下降，如果出现类似**弧形**的曲线，那么解决方案是**减小学习率**。

通常可以考虑尝试些学习率：  a =0.001，0.003，0.01，0.03，0.1，0.3，1，3，10

## 随机梯度下降(SGD)

观察BGD的公式，需要使用所有样本来更新参数，不适用于数据规模较大的时候

随机抽取一个样本来计算梯度，**收敛速度更快**

![image-20250403090707246](./assets/image-20250403090707246.png)

缺点是**局部最优**

## 小批量梯度下降(mini-batch)

每次使用**一部分样本**来计算梯度



<img src="./assets/image-20250403091847701.png" alt="image-20250403091847701" style="zoom:67%;" />

缺点是学习率不好调



# 正则化

正则化通过**限制模型的复杂度**，防止模型对训练数据拟合的**过于精细**，而无法泛化得到新数据，当模型过于复杂时，它不仅学到了数据的**真实模式（patterns）**，还学到了**噪声（noise）和细节**。

正则化的基本思想：在损失函数中加入**额外的惩罚项**，限制模型的参数规模，使其学习**更加平滑和简单**



噪声指学习中**随机的无意义的**变化，复杂的模型可能会记住噪声，可以通过降维或者正则化排除噪声的影响。

## L2正则化

 **L2 正则化（Ridge Regression / 权重衰减）**，在损失函数中添加了**模型参数的平方和惩罚项**：$J(\theta) = \text{Loss} + \lambda \sum_{i=1}^{n} \theta_i^2$

其中：

- **$\lambda$** 是正则化强度的超参数，控制惩罚项的大小。
- **$\sum_{i=1}^{n} \theta_i^2$** 约束权重，使它们尽可能小。

**作用：**

- **特征少，但所有特征都很重要**



## L1正则化

 **L1 正则化（Lasso Regression）**L1 正则化在损失函数中添加了**模型参数的绝对值和惩罚项**：$J(\theta) = \text{Loss} + \lambda \sum_{i=1}^{n} |\theta_i|$

**作用：**

- 可以将一些权重 $\theta_i$ 直接收缩到 0，实现特征选择（**即让不重要的特征系数变为 0**）。
- **适用于特征较多，希望选择重要特征**



# 归一化

一种将特征数据**缩放**到**相似数值范围**的预处理方法

常见的归一化有

+ 最小组最大值归一化，将数据压缩到[0,1]之间

**![image-20250506163548191](./assets/image-20250506163548191.png)**

+ Z-score标准化，将数据转化为均值为0，标准差为1

**![image-20250506163634521](./assets/image-20250506163634521.png)**

# 模型



## 线性回归

### 单变量线性回归

<img src="./assets/image-20250318093149416.png" alt="image-20250318093149416" style="zoom:80%;" />

<img src="./assets/image-20250318093137560.png" alt="image-20250318093137560" style="zoom:80%;" />

```python
path='ex1data1.txt'

# 读取数据
data=pd.read_csv(path,header=None,names=['Population','Profit'])

# 插入一行，作为偏移量
data.insert(0,'Ones',1)

cols=data.shape[1]

# 提取出输出和输出，转换为矩阵
X=np.matrix(data.iloc[:,:cols-1])
Y=np.matrix(data.iloc[:,-1:])

# 参数以一维形式初始化，计算时需要转置。第一个对应 偏移量的参数，第二个对于X的参数
theta=np.matrix(np.array([0,0]))

# 代价函数
def computCost(X,Y,theta):
    res=np.power((X*theta.T)-Y,2)
    return np.sum(res)/(2*len(X))


def batchGredientDescent(X,y,theta,iters,a):
    # 储存临时计算出的参乎上
    temp=np.matrix(np.zeros(theta.shape))
    # 储存每一轮迭代的代价
    cost=np.zeros(iters)
    # 参数个数
    parameter=int(theta.ravel().shape[1])
    
    # 迭代
    for i in range(iters):
        # 公式中的共有项
        error=(X*theta.T-y)
        
        # 同时更新
        for j in range(parameter):
            # 第一项是1不会有影响，因此正好更新了第二项
            term=np.multiply(error,X[:,j])
            # 公式求解
            temp[0,j]=theta[0,j]-(a/len(X))*np.sum(term)

        theta=temp
        cost[i]=computCost(X,Y,theta)
    return theta,cost


iters=1000
a=0.01

# 求解出最优参数
g,cost=batchGredientDescent(X,Y,theta,iters,a)

res=computCost(X,Y,g)

# 构图
fig,ax=plt.subplots(figsize=(12,8))
x=np.linspace(data['Population'].min(),data['Population'].max(),100)

ax.plot(x,g[0,0]+(g[0,1]*x),'r',label='Predit data')
ax.scatter(data['Population'],data['Profit'],label='Traning Data')
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.legend(loc=2)
ax.set_title('Predict data vs Population size')

plt.show()


fig,ax=plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters),cost,'r')
ax.set_xlabel('Iteration')
ax.set_ylabel('Cost')
ax.set_title('Error vs traning Eproch')
plt.show()
```

<img src="./assets/image-20250318093212503.png" alt="image-20250318093212503" style="zoom:67%;" />

通过观察代价函数在每一次迭代中的结果，可以检查算法是否正常工作

<img src="./assets/image-20250318093229085.png" alt="image-20250318093229085" style="zoom:67%;" />



### 多变量线性回归

在单变量的基础上多了一些参数

```python
path='ex1data2.txt'
data=pd.read_csv(path,header=None,names=['area','cnt','price'])

data=(data-data.mean())/data.std()
data.insert(0,'Ones',1)

cols=data.shape[1]

x=data.iloc[:,0:-1]
y=data.iloc[:,-1:]

theta=np.matrix(np.array([0,0,0]))

X=np.matrix(x.values)
Y=np.matrix(y.values)

# 代价函数
def computCost(X,Y,theta):
    res=np.power((X*theta.T)-Y,2)
    return np.sum(res)/(2*len(X))
def batchGredientDescent(X,y,theta,iters,a):
    # 储存临时计算出的参乎上
    temp=np.matrix(np.zeros(theta.shape))
    # 储存每一轮迭代的代价
    cost=np.zeros(iters)
    # 参数个数
    parameter=int(theta.ravel().shape[1])
    
    # 迭代
    for i in range(iters):
        # 公式中的共有项
        error=(X*theta.T-y)
        
        # 同时更新
        for j in range(parameter):
            # 第一项是1不会有影响，因此正好更新了第二项
            term=np.multiply(error,X[:,j])
            # 公式求解
            temp[0,j]=theta[0,j]-(a/len(X))*np.sum(term)

        theta=temp
        cost[i]=computCost(X,Y,theta)
    return theta,cost

iters=1000
a=0.01

g,cost=batchGredientDescent(X,Y,theta,iters,a)

fig,ax=plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters),cost,'r')
ax.set_xlabel('Iteration')
ax.set_ylabel('Cost')
plt.show()
```

<img src="./assets/image-20250318100633022.png" alt="image-20250318100633022" style="zoom:67%;" />



### 库函数实现线性回归

```python
from sklearn import linear_model
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error, mean_absolute_error
# 导入数据集
hs_p= fetch_california_housing()

X=hs_p.data
y=hs_p.target

print(y)

# 转换为Dataframe类型方便插入
X=pd.DataFrame(X)

# 线性回归需要插入截距项
X.insert(0,-1,1)

# 导入模型
model=linear_model.LinearRegression()

# 训练数据，可以传入DataFrame和array类型，可以混搭
model.fit(X.iloc[:20000,:],y[:20000])

# 使用训练好的数据预测
res=model.predict(X.iloc[20000:,:])

# 比较误差，越小越好
print(mean_absolute_error(y[20000:],res))
print(mean_squared_error(y[20000:],res))
```



### 正规方程

在n较大的时候运行较慢(**1e3**的数量级，复杂度来自于矩阵的逆变换)，但是**不需要迭代**。

对于不可逆的矩阵，正规方程是不可用的





补充矩阵求导公式：
![image-20250319102651355](./assets/image-20250319102651355.png)



![image-20250319102730863](./assets/image-20250319102730863.png)





<img src="./assets/image-20250319102740171.png" alt="image-20250319102740171" style="zoom:67%;" />





对于原公式

![image-20250319095540055](./assets/image-20250319095540055.png)

转换为矩阵形式有

![image-20250319095656781](./assets/image-20250319095656781.png)





展开求导，注意这里相乘的顺序不能颠倒

![image-20250319102815798](./assets/image-20250319102815798.png)



最后得到

<img src="./assets/image-20250319102940856.png" alt="image-20250319102940856" style="zoom:67%;" />



直接套公式求解

```python
# 注意矩阵matrix已被弃用
X=np.array(x.values)
Y=np.array(y.values)

res=np.linalg.inv(X.T@X)@X.T@Y
```





## 逻辑回归

逻辑回归的假设模型是：![image-20250319111639081](./assets/image-20250319111639081.png)**对于给定的输入变量，根据选择的参数计算输出变量等于1的可能**



其中g是sigmod函数，![image-20250319111721161](./assets/image-20250319111721161.png)

![image-20250319111716735](./assets/image-20250319111716735.png)

当g(z)>=0.5时预测y=1，反之y=0，根据图像可知，z求出的结果>=0时y=1，反之y=0



因此当给定参数时，通过与0比较大小可以得到一条决策边界。

![image-20250319121645508](./assets/image-20250319121645508.png)







逻辑回归的代价函数定义为

![image-20250319124333428](./assets/image-20250319124333428.png)

从图像上理解：当目标值y是1，h(x)的结果表示输出量是1的可能，当可能越大代价函数就越小，当h(x)趋近于0时，代价函数也会越来越大(因为与正确结果相悖)

<img src="./assets/image-20250319124339777.png" alt="image-20250319124339777" style="zoom:67%;" />

<img src="./assets/image-20250319124558257.png" alt="image-20250319124558257" style="zoom:67%;" />



将式子合并有

![](./assets/image-20250320091919821.png)



使用梯度下降，代价函数有



<img src="./assets/image-20250320094417716.png" alt="image-20250320094417716" style="zoom:80%;" />



最后有

![image-20250320094443867](./assets/image-20250320094443867.png)





<img src="./assets/image-20250320121714444.png" alt="image-20250320121714444" style="zoom:67%;" />



注意下述的是实现中使用Truncated Newton Conjugate-Gradient（TNC）算法——截断牛顿共轭梯度(原理较为复杂)来寻找最优参数

fmin_tnc(func=cost,x0=theta,fprime=gradient,args=(x,y))，其中theta必须是行向量形式，且cost和fprime的第一个参数必须是theta

```python
filename='ex2data1.txt'
data=pd.read_csv(filename,header=None,names=['Exam1','Exam2','Admitted'])

# 按照类别划分数据
pos=data[data['Admitted']==1]
neg=data[data['Admitted']==0]

# 绘图
fig,ax=plt.subplots(figsize=(12,8))
ax.scatter(pos['Exam1'],pos['Exam2'],color='blue',label='AC')
ax.scatter(neg['Exam1'],neg['Exam2'],color='r',marker='x',label='WA')
ax.legend()

# plt.show()

# sigmod函数
def sigmod(x):
    return 1/(1+np.exp(-x))

# 常数项
data.insert(0,'Ones',1)
cols=data.shape[1]

# 正则化
data.iloc[:,1:-1]=(data.iloc[:,1:-1]-data.iloc[:,1:-1].mean())/data.iloc[:,1:-1].std()

# 输出和输出 X.shape, y.shape分别是((100, 3), (100, 1))
x=np.array(data.iloc[:,:cols-1].values)
y=np.array(data.iloc[:,cols-1:].values)

# 注意参数初始化为行向量，在计算时可以通过reshape(-1,1)变为列向量
theta=np.zeros(cols-1)

# 代价函数
def cost(theta,X,y):
    # 注意维度
    return -np.sum(y*np.log(sigmod(X@theta.reshape(-1,1)))+(1-y)*np.log(1-sigmod(X@theta.reshape(-1,1))))/len(X)

# 计算步长，没有执行梯度下降
def gradient(theta,X,y):
    paramater=theta.shape[0]
    term=np.zeros(paramater)
    error=(sigmod(X@theta.reshape(-1,1))-y)

    for j in range(paramater):
        # 下面是向量乘法
        term[j]=np.sum(error*X[:,j].reshape(-1,1))/len(X)
    return term

# fmin_tnc是一种常用的寻找最优参数的算法
# 按照参数的要求待机函数和梯度函数中第一个参数必须要(n,)的参数向量，其他的参数通过args用元组的形式传入
best_theta,_,_=opt.fmin_tnc(func=cost,x0=theta,fprime=gradient,args=(x,y))

# 给定参数和输入预测结果
def predict(X,theta):
    possible=sigmod(X@theta.reshape(-1,1))
    return [1 if x>=0.5 else 0 for x in possible]

# 比较预测值和真实值
res=predict(x,best_theta)
compare=[ 1 if i==j  else 0 for i,j in zip(res,y)]
accury=sum(compare)/len(compare)
print(accury)
```



注意逻辑回归一次只能处理二分类问题，因为我们根据h(x)的输出，来划分1和0，因此对于多个类别，需要为**每个类别训练一个模型**，求解某个类别时使用对应的模型来处理数据。

<img src="./assets/image-20250320123622836.png" alt="image-20250320123622836" style="zoom:80%;" />

<img src="./assets/image-20250320123615651.png" alt="image-20250320123615651" style="zoom:80%;" />

[多分类的逻辑回归](./个人代码实现/ex3.html)





## 支持向量机(SVM)

基本思想：在二分类问题上，找到一个超平面，使得使两类数据离这个超平面越远越好，对于线性可分的数据来说，超平面有无数多个，但是**几何间隔最大的只有一个**。

当数据无法在**原始特征空间**中用一条直线(二维)，或一个平面(三维)分开时，就需要用**核函数**把数据**映射到更高维的空间**。

只适合于**小样本**



代码如下[ex6](./个人代码实现/ex6.html)



## 岭回归Ridge和Lasso回归

岭回归是在线性回归的基础上加上L2正则化，Lasso则是加上L1正则化。







## K-means

K-means是一个**迭代式**算法

1. 首先**随机选择K个点**，对于数据集中的每一个数据，按照**距离K个中心点的距离**，将其与**最近的中心点**关联起来，与**同一个中心点关联起来的点聚为一类**。
2. 计算每一类的**平均值**，然后**移动该类中心点到均值处**
3. 不断重复上述步骤



K-means的代价函数是**最小化所有数据点与其中心点的距离之和**

K-means还可以用于图片压缩，**通过将同一类的像素点转换为相同颜色，减小颜色的种类**



缺点是**收敛速度慢，对异常值敏感**，可能因为**会停留到局部最小值**(取决于初始化的中心点)，一般的解决办法是**多次运行**K-means算法取其中**代价最小**的结果。

代码如下[k_means](./个人代码实现/K_means.html)



## 主成分分析PCA

**PCA所做的就是找到一个方向向量，当把所有数据映射到该向量上时，得到的误差最小**

注意与线性回归不同

<img src="./assets/image-20250331092051938.png" alt="image-20250331092051938" style="zoom:80%;" />

将数据从n维降到k维的过程如下：

1. **均值归一化**
2. **计算协方差矩阵$\Sigma = \frac{1}{m} \sum_{i=1}^n \left( x^{(i)} \right) \left( x^{(i)} \right)^T$**，协方差矩阵的维度是(n,n)即特征数决定
3. **计算协方差矩阵的特征向量，经过SVD分解，得到U(数据在原始空间的投影方向) S V ，取出前k个U**
4. **计算新的特征值$Z^{(l)} = U_{reduce}^T \cdot \chi^{(l)}$**



选择主成分数量最常用的方法是**保留累计方差贡献率**

$X_{approx}^{(i)}=U_{reduce}z^{(i)}$是PCA **降维**后**再映射回原始空间**的近似数据点，这个比值是**数据降维后损失的方差信息占总方差的比例**即**重建误差**，一般要求不超过1%。

![image-20250331100957096](./assets/image-20250331100957096.png)



恢复后的数据是会损失信息的

<img src="./assets/image-20250401104417062.png" alt="image-20250401104417062" style="zoom: 33%;" /><img src="./assets/image-20250401104358157.png" alt="image-20250401104358157" style="zoom: 33%;" />



如果SVD分解后的**S矩阵**(协方差矩阵的特征值)**只有对角线上有值**，那么可以表示为

![image-20250331101554819](./assets/image-20250331101554819.png)

进行交叉验证时，使用**训练集上得到的参数进行PCA压缩**，而不是**整合所有的数据或者单独对验证集**进行PCA

PCA并不是一个很好的解决**过拟合**的方法，一般还是用正则化

**注意PCA不应作为学习过程中必要的一步**



代码如下[PCA](./个人代码实现/pca.html)



## KNN

给定一个数据集，对于新的输入实例，在训练集中找到与该实例**最邻近的K个实例**，这K实例的**多数**属于某个类，就把这个输入实例分类这个类中(**少数服从多数**)。

![img](./assets/v2-c3f1d2553e7467d7da5f9cd538d2b49a_1440w.png)

如上图所示，如果K=3，那么新的实例绿色就属于红色，如果K取到5，那么就属于蓝色，这里得距离度量可以是欧氏距离或者曼哈顿距离。



K值得选取需要通过**调参**获得，在训练模型之前需要进行**归一化**处理。



## 决策树

决策树是一种树形结构，内部节点表示一个属性上的判断，分支表示判断结果的输出，叶子节点表示分类结果，决策树可以看作是if-else规则的集合。

![img](./assets/v2-ff4fe0d16ec17c5520837b3aad52ed54_1440w.jpg)



决策树的分类算法有

- **信息增益（ID3算法）**
- **信息增益率（C4.5算法）**
- **基尼指数（Gini Index，CART算法）**

### 剪枝

决策树不断划分的过程中会越来越复杂，导致过拟合，通过剪枝可以减低模型复杂度，避免过拟合。

剪枝有两种：后剪枝，预剪枝。

+ 后剪枝：构建出一棵树后，从底往上找，如果将某个子树替换为叶子节点，模型的表现会更好，就进行剪枝
+ 预剪枝：如果节点划分不能明显提示信息增益就不再划分

### ID3

ID3，在每个节点选择**信息增益**最大的特征进行划分

+ **信息熵：随机变量的不确定性**
+ **信息增益：根据某特征划分数据后，信息熵的减少量**



算法的流程是

1. **对于当前样本集合求解出所有属性信息的信息增益**
2. **按照信息增益最大的特征划分样本**
3. **递归处理子样本集**



| 优点                   | 缺点                                           |
| ---------------------- | ---------------------------------------------- |
| **结构直观，较为清晰** | **容易过拟合**                                 |
|                        | **无法处理连续值，缺失值**                     |
|                        | **倾向于多值特征，因为信息增益倾向于多值特征** |



ID3**不能剪枝**，因为设计之初就没想过剪枝



### C4.5

在ID3基础上提出的改进型决策树算法，专用于分类任务，解决了一些ID3的缺陷，比如对多值属性偏好、不能处理连续属性、不能剪枝等问题。

C4.5使用**信息增益率**代替信息增益，可以**避免偏好多值特征**。

C4.5采用**后剪枝**



### 基尼指数

基尼指数是CART(Classification and Regression Tree)中用于特征选择的重要指标，用于衡量数据的纯度，它反映**从数据集中随机选取两个样本，它们属于不同类别的概率**，因此基尼指数越小，数据越纯净。

每次选择**加权基尼指数最小**的作为划分特征。

![image-20250507220545378](./assets/image-20250507220545378.png)

基尼指数采用**后剪枝或预剪枝**

## 随机森林





## GDBT

梯度提升决策树，使用的是Boosting思想。

Booting思想是一种**集成学习**方法，其核心思想是：**将多个弱分类器通过加权组合，提升为一个强分类器。**



















# 神经网路

神经网络模型大致如下，每一层都会包含一个偏置项，但是偏置项是没有输出连接的。

![Network331.png](./assets/bfd3e060b4ebe5e31145e4924053633f.png)

第一层称为**输入层**，最后一层是**输出层**，中间的称为**隐藏层**，隐藏层中的每个单元称为**激活单元**，其采纳一些特征作为输入，并根据**自身模型**提供一个输出。

这是最基本的多层感知机**MLP**，或者说全连接神经网络



![\textstyle (W,b) = (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)})](./assets/120355d321345cf7d14886639147f127.png)

其中 ![\textstyle W^{(l)}_{ij}](./assets/425eb7f5f59daf9f05afa431348fd4d7.png) 是第 ![\textstyle l](./assets/a2bba02c6a7826a91789d70429382c2d.png) 层第 ![\textstyle j](./assets/b5b9dd7984ddb38491e0fd22ca643e48.png) 单元与第 ![\textstyle l+1](./assets/7f54105bf3106a0cbc2ef415ff39fefe.png) 层第 ![\textstyle i](./assets/e7fac7a0264acd0558200fe1e902a09e.png) 单元之间的联接参数（其实就是连接线上的权重，注意标号顺序）， ![\textstyle b^{(l)}_i](./assets/12646d34b647e4fc9ec869a718cf9235.png) 是第 ![\textstyle l+1](https://i-blog.csdnimg.cn/blog_migrate/7f54105bf3106a0cbc2ef415ff39fefe.png) 层第 ![\textstyle i](https://i-blog.csdnimg.cn/blog_migrate/e7fac7a0264acd0558200fe1e902a09e.png) 单元的偏置项。



计算过程如下：
![](./assets/image-20250324091258145.png)





一般用用 ![\textstyle z^{(l)}_i](./assets/28f05c3b81022a86e83875f86c34dfb2.png) 表示第 ![\textstyle l](./assets/a2bba02c6a7826a91789d70429382c2d-1742778974573-21.png) 层第 ![\textstyle i](./assets/e7fac7a0264acd0558200fe1e902a09e-1742778974573-23.png) 单元输入加权和（包括偏置单元），比如， ![\textstyle  z_i^{(2)} = \sum_{j=1}^n W^{(1)}_{ij} x_j + b^{(1)}_i](./assets/82c4583ce05a6ec7d1d9fcaeb34a8a4c.png) ，则 ![\textstyle a^{(l)}_i = f(z^{(l)}_i)](./assets/3c5e91857bd0fcf0420c251fe6c81218.png) 。

这样就有更简洁的表示方法了

![\begin{align}z^{(2)} &= W^{(1)} x + b^{(1)} \\a^{(2)} &= f(z^{(2)}) \\z^{(3)} &= W^{(2)} a^{(2)} + b^{(2)} \\h_{W,b}(x) &= a^{(3)} = f(z^{(3)})\end{align}](./assets/563b66c6d3362528ead8db5a0ad540e2.png)



上述的计算步骤就是前向传播，只要给定前一层的激活值，那么后一层就可以递推出来

![\begin{align}z^{(l+1)} &= W^{(l)} a^{(l)} + b^{(l)}   \\a^{(l+1)} &= f(z^{(l+1)})\end{align}](./assets/7113e4d8c3bf152de4fde4157841c66b.png)



每一层求解出的特征是**神经网络通过学习**得到的一系列用于预测变量的**新特征**，这些特征值比将输入组合为高次项更为**有效**。

假设X有(5000,400)，这里的$$a_1^{(1)}$$是(1,400)的**行向量**也就是训练并不会修改**输入的个数**，**而是在输入的特征个数上拟合训练**



假设神经网络的训练样本有$m$个，每个包含一组输入$x$和一组输出信号$y$，$L$表示神经网络层数，$S_I$表示每层的**neuron**个数($S_l$表示输出层神经元个数)，$S_L$代表最后一层中处理单元的个数。

将神经网络的分类定义为两种情况：**二类分类和多类分类**，

二类分类：$S_L=0, y=0\, or\, 1$表示哪一类；

$K$类分类：$S_L=k, y_i = 1$表示分到第$i$类；$(k>2)$，最后的输出层有**k个节点**，经过sigmoid的激活函数得到一个向量，表示当前输入对应的每个分类的可能。

![8f7c28297fc9ed297f42942018441850](./assets/8f7c28297fc9ed297f42942018441850-1742777522922-1.jpg)





$J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log {(h_\Theta(x^{(i)}))}{_k} + \left( 1 - y_k^{(i)} \right) \log \left( 1- {\left( h_\Theta \left( x^{(i)} \right) \right)}{_k} \right) \right] + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1}} \left( \Theta_{ji}^{(l)} \right)^2$。

正则项实际就是**所有的参数除了每一层的偏置参数的平方和**



## 反向传播算法BP

在预测神经网络使用**正向传播方法**，即从第一层开始正向一层一层计算，直到最后一层h(x)

而为了计算代价函数的偏导$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J\left(\Theta\right)$，需要采用一种**反向传**播的方法，首先计算出**最后一层误差**，然后一层一层反向求出误差，直到**倒数第二层**(第一层是输入没有误差)。



1. 先计算每一层的激活值和状态，直到最后一层(前向传播)
2. 从最后一层计算误差，并向前推进
3. 计算每个神经元连接权重的梯度
4. 更新权重



假设我们的训练集只有一个样本$\left({x}^{(1)},{y}^{(1)}\right)$，我们的神经网络是一个四层的神经网络，其中$K=4，S_{L}=4，L=4$：

前向传播算法：



<img src="./assets/image-20250325083800047.png" alt="image-20250325083800047" style="zoom:67%;" />



令总体代价为

<img src="./assets/image-20250325083936846.png" alt="image-20250325083936846" style="zoom:50%;" />

目标是调整权重使得代价最小，通过求偏导来使用梯度下降‘

<img src="./assets/image-20250325084226667.png" alt="image-20250325084226667" style="zoom:67%;" />



用链式法则求解输出层的误差，有

<img src="./assets/image-20250325085155622.png" alt="image-20250325085155622" style="zoom:50%;" />

<img src="./assets/image-20250325085211309.png" alt="image-20250325085211309" style="zoom: 50%;" />

这里$对z求偏导是因为z是计算出的真实值，而a是经过激活函数后的结果$



因此输出层的误差有

<img src="./assets/image-20250325085236026.png" alt="image-20250325085236026" style="zoom:67%;" />

<img src="./assets/image-20250325085645860.png" alt="image-20250325085645860" style="zoom:67%;" />



输出层误差求导后有

<img src="./assets/image-20250325085726948.png" alt="image-20250325085726948" style="zoom: 67%;" />

<img src="./assets/image-20250325090429764.png" alt="image-20250325090429764" style="zoom:67%;" />



最后有输出层的误差

<img src="./assets/image-20250325090437325.png" alt="image-20250325090437325" style="zoom:67%;" />



对于隐藏层有

<img src="./assets/image-20250325094511625.png" alt="image-20250325094511625" style="zoom:67%;" />



因此隐藏层的误差为

<img src="./assets/image-20250325095002952.png" alt="image-20250325095002952" style="zoom:50%;" />



更新权重

<img src="./assets/image-20250325095050391.png" alt="image-20250325095050391" style="zoom:50%;" />



<img src="./assets/image-20250325095042128.png" alt="image-20250325095042128" style="zoom:50%;" />



可得参数更新公式为

<img src="./assets/image-20250325095150628.png" alt="image-20250325095150628" style="zoom:67%;" />



当代价函数为交叉熵，即$$J\left(\theta\right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log\left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}$$，且激活函数为sigmoid时

<img src="./assets/image-20250325101926592.png" alt="image-20250325101926592" style="zoom:67%;" />

<img src="./assets/image-20250325101933822.png" alt="image-20250325101933822" style="zoom:67%;" />

输出层的损失函数化简为简洁的形式

<img src="./assets/image-20250325123602664.png" alt="image-20250325123602664" style="zoom:67%;" />



在实际实现中会有很多个输入变量，会将所有输入变量计算出的梯度**累加取平均**

公式推导见[解读反向传播算法（图与公式结合） - 知乎](https://zhuanlan.zhihu.com/p/96046514)

代码实现见[ex4](./个人代码实现/ex4.html)



## RNN

RNN循环神经网络，前向反馈的神经网络只能单独处理一个个输入，前一个输入和后一个输入之间是完全没有关系的，但是某些任务需要更好的处理序列信息，即前一个输入后一个输入之间是有关系的。



<img src="./assets/image-20250502090831997.png" alt="image-20250502090831997" style="zoom: 33%;" />

<img src="./assets/image-20250502091104798.png" alt="image-20250502091104798" style="zoom: 67%;" />

隐藏层的值不仅取决于输入，还取决于上一时刻的隐藏层的值

<img src="./assets/image-20250502091318277.png" alt="image-20250502091318277" style="zoom:50%;" />

<img src="./assets/image-20250502091833366.png" alt="image-20250502091833366" style="zoom:50%;" />





## CNN

卷积神经网络，Convolutional Neural Networks

使用全连接神经网络处理大尺寸图象时，有三个明显的缺点：

1. 将图像展开为向量会丢失空间信息(平移不变性，旋转不变性，明暗不变性，缩放不变性)
2. 参数过多效率低下
3. 大量的参数会导致过拟合



CNN的基本结构：

1. 图像就是输入层
2. CNN的卷积层
3. CNN的池化层，卷积层+池化层可以在隐藏层中出现很多次，也可以灵活组合，卷积+卷积+池化，卷积+卷积
4. 全连接层



卷积操作将具有识别某种特征能力的权重共享到其他位置上，大致过程类似滑动窗口

实际上也是在做全连接操作，但是参数数量急剧减小

<img src="./assets/v2-4fd0400ccebc8adb2dffe24aac163e70_b.webp" alt="动图" style="zoom:67%;" />

卷积操作的过程：每次取一个特定大小的矩阵F(蓝色阴影)，然后将其与X(蓝色矩阵)依次扫描并进行内积运算，扫描结束后得到的了整个卷积后的结果Y(绿色矩阵)。

![动图](./assets/v2-5efbada1d4bb599e6195ce819aa7ebd9_b.webp)

其中F被称为卷积核 convolutional kernel或者叫filter，可以有多个，卷积后的结果Y称为特征图feature map，每一个卷积核卷积后都会得到一个对应的特征图。最后对于特征X会使用三个维度进行表示——宽吗，高和通道channel。



感受野指的是每一层输出的特征图上的像素点映射回输入图像的区域大小，网络越深感受野就越大。





多核卷积就是有多个卷积核，因为一个卷积核可以识别某一类元素，而一个图像往往有多类特征。

现已单通道单卷积核为例

<img src="./assets/v2-41ea774397ab503144f2e410e3b91846_1440w.jpg" alt="img" style="zoom:50%;" />

计算过程如下

<img src="./assets/image-20250502110445319.png" alt="image-20250502110445319" style="zoom: 50%;" />

<img src="./assets/v2-f3cbc9092253d32f9faef27431094794_r.jpg" alt="img" style="zoom:67%;" />



<img src="./assets/image-20250502110513383.png" alt="image-20250502110513383" style="zoom:67%;" />

<img src="./assets/v2-34b0159c662de8c287e47afa4e05351b_1440w.jpg" alt="img" style="zoom:67%;" />



单通道多核

<img src="./assets/v2-28512602aa1cc0d5cbbd3728f1b71bd0_1440w.jpg" alt="img" style="zoom: 50%;" />



<img src="./assets/v2-867e4addafd9d5146d7be731cd94a8d5_1440w.jpg" alt="img" style="zoom:50%;" />



多通道单核

这里的卷积核也是多通道的

<img src="./assets/v2-74b6e3cb4f6f1714e05325c068ece3e6_1440w.jpg" alt="img" style="zoom:50%;" />

<img src="./assets/v2-b9b793a795f06aa1830a784ad82ccf14_1440w.jpg" alt="img" style="zoom:50%;" />

<img src="./assets/image-20250502110807526.png" alt="image-20250502110807526" style="zoom:50%;" />



多通道多核

类比单通道多核

<img src="./assets/v2-a49ca8fccf9423b68350a2948c58df69_1440w.jpg" alt="img" style="zoom:50%;" />



<img src="./assets/v2-1ba1b27de724a353656802d32d41c891_1440w.jpg" alt="img" style="zoom:50%;" />



+ 原始输入有多少个通道，卷积核就必须有多少个通道
+ K个卷积核对输入进行卷积操作，那么最后得到的特征图就一定包含K个通道



深度卷积：卷积之后再卷积，获取更高级更抽象的特征。



通常在连续的卷积层之间会周期性地插入一个池化层Pooling，它的作用是逐渐降低数据体的空间尺寸，这样就能减少网络中参数的数量，使得计算资源的耗费减少，也能有效控制过拟合。

池化最常见的有最大池化和平均池化，前者更常用

<img src="./assets/image-20250502132326977.png" alt="image-20250502132326977" style="zoom:50%;" />



# 误差分析

**有时准确率不能作为评判性能的标准**——比如对于一个只有**5%的数据为false**的数据集，一个只会输出1的函数的准确率可以达到95%，比学习算法得到结果更好，这类数据集称为**偏斜类**（skewed classes），准确率是不能用于评价模型好坏的。

定义**查准率(precision)**和**查全率(recall)**，以及四种预测结果

1. **正确肯定（True Positive,TP）：预测为真，实际为真** 
2. **正确否定（True Negative,TN）：预测为假，实际为假** 
3. **错误肯定（False Positive,FP）：预测为真，实际为假** 
4. **错误否定（False Negative,FN）：预测为假，实际为真**



查准率=**TP/(TP+FP)**例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，**越高越好**

查全率=**TP/(TP+FN)**例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的 病人的百分比，**越高越好**



如果更在意**查准率**，那么就**提高阈值**，反之**降低阈值**

对于想要**平衡**二者的分类模型，选择能得到**最高F1 Score**的阈值

![image-20250328130301305](./assets/image-20250328130301305.png)







# 异常检测

通过**现有的样本**来训练一个架构，它能够根据输入与现有样本之间是否足够**相似**，来判断这个输入是否是异常的

异常检测有点类似于二分类，即其中有**正类(正常)和负类(异常)**，异常检测中**正类很容易收集到，但是负类很难收集或者说无穷无尽没有办法学习到每一个负类的特征**。



异常检测使用**高斯分布**——正态分布

对于给定的数据集，需要求出**每个特征的均值和方差**

<img src="./assets/image-20250401131554697.png" alt="image-20250401131554697" style="zoom:67%;" />





对于一个新的训练实例，**累乘其各个特征的正态分布值**，当𝑝(𝑥) < 𝜀时，为异常

![image-20250401131630878](./assets/image-20250401131630878.png)



高斯分布的过程是：将数据分为训练集和测试集，通过测试集数据估计出各个特征的均值和方差，对于验证集，采用不同的𝜀，根据f1值选择𝜀。

异常检测算法最好将数据转换为**高斯分布**，例如使用对数函数log(x+c)，c是非负常数，或者$x^c$，c介于0-1之间

<img src="./assets/image-20250402085343275.png" alt="image-20250402085343275" style="zoom:67%;" />

有的时候异常的数据也可能有较高的p(x)值，因而被算法认为是正常的，这时候需要添加一些新的特征来帮助检测，比如组合现有的特征。



当特征之间存在**相关性**，高斯分布不能很好的检测出异常(可以通过构建**组合特征**来捕捉相关性)，而多元高斯分布可以**自动捕捉**特征之间的相关性

首先计算所有特征的平均值，然后计算协方差矩阵

<img src="./assets/image-20250402091945249.png" alt="image-20250402091945249" style="zoom:50%;" />

然后计算p(x)

![image-20250402092013807](./assets/image-20250402092013807.png)



多元高斯分布的代价比较高且必须要满足**m>>n（m>10n）**以此来保证协方差矩阵可逆，此外还要求没有冗余特征



代码实现如下[高斯分布](./个人代码实现/高斯分布.html)



